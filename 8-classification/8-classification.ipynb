{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution of https://github.com/apohllo/nlp/blob/master/8-classification.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "import shutil\n",
    "from random import sample\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bills(directory_path: str):\n",
    "    files = [file for file in listdir(directory_path) if isfile(join(directory_path, file))]\n",
    "    for file in files:\n",
    "        with open(f\"{directory_path}/{file}\") as input_file:\n",
    "            content = input_file.read()\n",
    "            content_1 = clean(content)\n",
    "            if is_ammending(content_1):\n",
    "                shutil.copy(f\"{directory_path}/{file}\", \"classifications/amending/\", follow_symlinks=True)\n",
    "            else:\n",
    "                shutil.copy(f\"{directory_path}/{file}\", \"classifications/normal/\", follow_symlinks=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ammending(content: str):\n",
    "    first_part = get_first_part(content)\n",
    "    return contains_ammendment(first_part)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_part(content: str):\n",
    "    splitted = content.split(' ')\n",
    "    again = \"\"\n",
    "    for word in splitted:\n",
    "        if word == \"Art.\":\n",
    "            break\n",
    "        else:\n",
    "            again += word + \" \" \n",
    "    return again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_part(content: str):\n",
    "    splitted = content.split(' ')\n",
    "    again = \"\"\n",
    "    take = False\n",
    "    for word in splitted:\n",
    "        if word == \"Art.\":\n",
    "            take = True\n",
    "        if take:\n",
    "            again += word + \" \" \n",
    "    return again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(content: str):\n",
    "    content_1 = content.replace(u'\\s', u' ')\n",
    "    content_2 = content_1.replace(u'\\t', u' ')\n",
    "    content_3 = content_2.replace(u'\\xa0', u' ')\n",
    "    return content_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_ammendment(text:str):\n",
    "    return regex.findall('[\\s]*o[\\s]*zmianie[\\s]*ustawy[\\s]*', get_first_part(text)) != []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_title(content:str):\n",
    "    return get_second_part(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_bills(directory_path: str, label:int):\n",
    "    files = [file for file in listdir(directory_path) if isfile(join(directory_path, file))]\n",
    "    for file in files:\n",
    "        with open(f\"{directory_path}/{file}\") as input_file:\n",
    "            content = input_file.read()\n",
    "            content_1 = clean(content)\n",
    "            content_2 = get_second_part(content_1)\n",
    "            if len(content_2.split('\\n')) < 10:\n",
    "                print(content_2)\n",
    "            else:\n",
    "                bill_tuple = (content_2, get_10_percent(content_2),get_10_lines(content_2), get_line(content_2))\n",
    "                bills.append(bill_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10_percent(content:str):\n",
    "    lines = content.split('\\n')\n",
    "    n_lines = len(lines)\n",
    "    ten_percent = n_lines // 10\n",
    "    ten_percent_lines = sample(lines, ten_percent)\n",
    "    separator = '\\n'\n",
    "    return separator.join(ten_percent_lines)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10_lines(content: str):\n",
    "    lines = content.split('\\n')\n",
    "    ten_lines = sample(lines, 10)\n",
    "    separator = '\\n'\n",
    "    return separator.join(ten_lines)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(content: str):\n",
    "    lines = content.split('\\n')\n",
    "    return choice(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def serialize(name, data):\n",
    "    with open(name, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def deserialize(name):\n",
    "    with open(name, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize('train', train)\n",
    "serialize('validate', validate)\n",
    "serialize('test', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_x, train_y, validate_x, validate_y, parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Best parameters set:\")\n",
    "    print (grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "    # measuring performance on test set\n",
    "    print (\"Applying best classifier on test data:\")\n",
    "    best_clf = grid_search_tune.best_estimator_\n",
    "    predictions = best_clf.predict(validate_x)\n",
    "    return best_clf, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_svm(train_text, train_label, validate_text, validate_label):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    clf, predictions = grid_search(train_text, train_label, validate_text, validate_label, parameters, pipeline)\n",
    "    pr_ = np.array(predictions)\n",
    "    val_ = np.array(validate_label)\n",
    "    print(sum(pr_==val_)/ len(pr_))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.7min\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=0.1, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "0.8914285714285715\n"
     ]
    }
   ],
   "source": [
    "clasiffier_full = tf_idf_svm(train[1], train[5], validate[1], validate[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f(clasifier, data):\n",
    "    predicted = clasifier.predict(data)\n",
    "    return precision_recall_fscore_support(test[5], predicted, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7912087912087912, 0.8674698795180723, 0.8275862068965518, None)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f(clasiffier_full, test[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:   42.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "0.7942857142857143\n"
     ]
    }
   ],
   "source": [
    "clasifier_10_precent = tf_idf_svm(train[2], train[5], validate[2], validate[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6836734693877551, 0.8072289156626506, 0.7403314917127072, None)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f(clasifier_10_precent, test[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.1371s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=3)]: Done  32 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=3)]: Done  46 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=3)]: Done  64 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=3)]: Done  82 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=0.01, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "0.7885714285714286\n"
     ]
    }
   ],
   "source": [
    "clasifier_10 = tf_idf_svm(train[3], train[5], validate[3], validate[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6637168141592921, 0.9036144578313253, 0.7653061224489797, None)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f(clasifier_10, test[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0567s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  42 tasks      | elapsed:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "clasifier_1 = tf_idf_svm(train[4], train[5], validate[4], validate[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46496815286624205, 0.8795180722891566, 0.6083333333333333, None)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f(clasifier_1, test[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastText(train_text, train_label, validate_text, validate_label, name):\n",
    "    with open(name, 'w+') as file:\n",
    "        for index, content in enumerate(train_text):\n",
    "            clean =  content.replace('\\n', \" \")\n",
    "            file.write(f\"__label__{np.array(train_label)[index]} {clean}\\n\")\n",
    "    classifier_f = fasttext.supervised(name, f'model_text_{name}')\n",
    "    return classifier_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasifier_full_fasttext = fastText(train[1], train[5], validate[1], validate[5], 'full.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clasifier_full_fasttext.predict(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f_fasttext(clasifier_f, data):\n",
    "    predicted_f = clasifier_f.predict(data)\n",
    "    predicted_numbers = list(map(lambda x: int(x[0]), predicted_f))\n",
    "    return precision_recall_fscore_support(test[5], predicted_numbers, average='binary')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4715909090909091, 1.0, 0.640926640926641, None)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f_fasttext(clasifier_full_fasttext, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasifier_10_percent_fasttext = fastText(train[2], train[5], validate[2], validate[5], '10percent.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4715909090909091, 1.0, 0.640926640926641, None)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f_fasttext(clasifier_10_percent_fasttext, test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasifier_10 = fastText(train[3], train[5], validate[3], validate[5], '10.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4715909090909091, 1.0, 0.640926640926641, None)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f_fasttext(clasifier_10, test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasifier_1 = fastText(train[4], train[5], validate[4], validate[5], '1.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4715909090909091, 1.0, 0.640926640926641, None)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f_fasttext(clasifier_1, test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
